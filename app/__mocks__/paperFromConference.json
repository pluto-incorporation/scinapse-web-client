{
  "id": "2406556600",
  "title": "Machine Learning in Adversarial Settings",
  "year": 2016,
  "published_date": "2016-05-01",
  "doi": "10.1109/MSP.2016.51",
  "volume": "14",
  "issue": "3",
  "author_count": 3,
  "reference_count": 7,
  "cited_count": 20,
  "journal": null,
  "conference_instance": {
    "id": "2334792722",
    "conference_series": {
      "id": "1163618098",
      "name": "IEEE Symposium on Security and Privacy",
      "paper_count": 3117,
      "citation_count": 143139
    },
    "name": "IEEE S&P 2016",
    "location": "San Jose, USA",
    "official_url": "http://www.ieee-security.org/TC/SP2016/",
    "start_date": "2016-05-23",
    "end_date": "2016-05-25",
    "abstract_registration_date": null,
    "submission_deadline_date": null,
    "notification_due_date": "2015-02-07",
    "final_version_due_date": null,
    "paper_count": 186,
    "citation_count": 2122
  },
  "authors": [
    {
      "id": 2056207806,
      "name": "Patrick D. McDaniel",
      "affiliation": {
        "id": 130769515,
        "name": "Pennsylvania State University"
      },
      "paper_count": 257,
      "citation_count": 13897,
      "order": 1,
      "hindex": 53,
      "profile_image_url": null,
      "is_layered": false
    },
    {
      "id": 248975517,
      "name": "Nicolas Papernot",
      "affiliation": {
        "id": 130769515,
        "name": "Pennsylvania State University"
      },
      "paper_count": 41,
      "citation_count": 1966,
      "order": 2,
      "hindex": 16,
      "profile_image_url": null,
      "is_layered": false
    },
    {
      "id": 2250297608,
      "name": "Z. Berkay Celik",
      "affiliation": {
        "id": 130769515,
        "name": "Pennsylvania State University"
      },
      "paper_count": 23,
      "citation_count": 738,
      "order": 3,
      "hindex": 7,
      "profile_image_url": null,
      "is_layered": false
    }
  ],
  "fos_list": [],
  "urls": [],
  "abstract": null,
  "title_highlighted": "<b>Machine</b><b> Learning</b> in Adversarial Settings",
  "abstract_highlighted":
    "Recent advances in <b>machine</b><b> learning</b> have led to innovative applications and services that use computational structures to reason about complex phenomenon. Over the past several years, the security and <b>machine</b><b>-learning</b> communities have developed novel techniques for constructing adversarial samples--malicious inputs crafted to mislead (and therefore corrupt the integrity of) systems built on computationally learned models. The authors consider the underlying causes of adversarial samples and the future countermeasures that might mitigate them.",
  "relation": null,
  "is_layered": false
}
